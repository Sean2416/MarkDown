# Docker

## 基本概念

### Docker Daemon

- ###### Docker Client通过命令与Docker Damon通信，完成Docker相关操作。Daemon的主要功能包括镜像管理、镜像构建、REST API、身份验证、安全、核心网络以及编排。

### IMAGE

- ###### Docker 映像檔就是一個唯讀的模板

- ###### 例如：一個映像檔可以包含一個完整的 ubuntu 作業系統環境，裡面僅安裝了 Apache 或使用者需要的其它應用程式。

- ###### 映像檔可以用來建立 Docker 容器。

- ###### Docker 提供了一個很簡單的機制來建立映像檔或者更新現有的映像檔，使用者甚至可以直接從其他人那裡下載一個已經做好的映像檔來直接使用。

- ###### `docker rmi $(docker images -q)` 快速清除所有image

### Container	

- ###### Docker 利用容器來執行應用

- ###### 容器是從映像檔建立的執行實例。它可以被啟動、開始、停止、刪除。每個容器都是相互隔離的、保證安全的平台。

- ###### 可以把容器看做是一個簡易版的 Linux 環境（包括root使用者權限、程式空間、使用者空間和網路空間等）和在其中執行的應用程式。

- ###### 映像檔是唯讀的，容器在啟動的時候建立一層可寫層作為最上層

- ###### `docker stop $(docker ps -aq)` 快速停止所有container

- ###### `docker rm $(docker ps -aq)` 快速清除所有container

- ##### 狀態

  - ###### Exited表示container暫時性地被關閉，並且不會在使用環境的任何資源，下次再啟動時，必須重新執行該container。
  
  - ###### Paused表示container暫時性地被暫停，但依舊會使用著環境資源(memories)，下次在unpause同時，會從上次暫停的地方繼續執行。
  
  - ![https://ithelp.ithome.com.tw/upload/images/20200919/20129737XZeelnsEOj.png](https://ithelp.ithome.com.tw/upload/images/20200919/20129737XZeelnsEOj.png)
  
    
  

### Registry

- ##### 將Image Push到Registry

  - ```powershell
    # login to docker hub (如果要 push 到 DockerHub, 若是 Private docker registry 則不需 login)
    docker login -u 帳號 -p 密碼
    # 將Image重新標註為上傳的Image檔案
    #確認 image 的前置詞需是自己的 docker user id (若是 Private docker registry 則需定義 domain ip 以及 Port)
    # ex. docker tag icw_cloud sean2416/docker-demo:latest
    docker tag [ImageName/ID] [DockerUserID]/[REPOSITORY_NAME]:[Tag]
    # push image to docker hub
    #docker push sean2416/docker-demo:latest
    docker push [DockerUserID]/[REPOSITORY_NAME]:[Tag]
    ```

  - ![img](https://raw.githubusercontent.com/Sean2416/Pic/master/img/202206172150475.png)

- ##### Private registry 安裝

  - ##### crceate private registry

  ```powershell
  # 下載 registry image
  docker pull registry
  # 建立 container
  # ex: docker run --name [私人註冊庫名稱] -p [外部 IP]:[內部 IP] -v [實體檔案路徑]:[Container 路徑]
  docker run --name icw-registry --restart always -p 5000:5000 -v D:/docker/registry:/var/lib/registry -d registry
  
  # 建立 WebUI 管理介面
  # docker run -d -p 8080:8080 --name registry-web --link [Name] -e REGISTRY_URL=http://[IP]:5000/v2 hyper/docker-registry-web
  docker run -d -p 8080:8080 --name registry-web --link icw-registry -e REGISTRY_URL=http://192.168.1.177:5000/v2 hyper/docker-registry-web
  
  # 需開啟防火牆 Port: 5000, 8080
  ```

  ### 本機(開發)端設定 (錯誤訊息: server gave HTTP response to HTTPS client)

  * docker config 設定
    * 進入 Docker 的 settings. 選擇 Daemon.
    * 於 Insecure registries 新增 registry 的 ip:port
  * restart docker

- ##### 參考

  - [ 上傳 Docker Image 到 Docker Hub](https://ithelp.ithome.com.tw/articles/10192824)

### Docker Volume

- ![img](https://myapollo.com.tw/images/docker-volumes/volume-types.png)

- ###### 讓 Docker Container 保存與共用資料的機制，原本 Docker Container 的資料只會存在該 container 內，並不會與外部或是其他 container 共享，如果在 container 運作時新增了一些資料，而在 container 移除時又沒有將該 container commit 成 image，則當 container 被刪除時，資料就會遺失。

- ###### 要避免資料遺失，就可以使用 Docker Volume。你可以建立一個 volume，將它指定到 container 內的某個目錄，這樣該目錄的資料就可以永續保存著，不會因為 container 移除而被移除。

- ##### Docker實現Volume的原理

  - ###### Volume: Container將Volume存放在Docker area，以Linux來說預設為Var/lib/docker/volume。

  - ###### BindMount: 可以為主機路徑下任何地方。

  - ###### tmpfsMount:主機的memory。

### Docker-compose

- ##### 可以將一群服務依據`yml`設定進行快速佈署

- 包含產生/pull image、建置container

### Docker-compose.yml

- ```yaml
  version: '3.4'
  
  services:
    catalogdb:
    #對應docker image名稱，沒有的話會去register下載
      image: mongo
  
    catalog.api:
      image: ${DOCKER_REGISTRY-}catalogapi
      build:
        context: .
        dockerfile: Services/Catalog/Catalog.API/Dockerfile
  
  #volumes會將container 產生的資料儲存
  volumes:
    mongo_data:
  ```

### Docker-compose.override.yml

- ```yaml
  version: '3.4'
  
  services:
    catalogdb:
      container_name: catalogdb
      restart: always
      ports:
          - "27017:27017"
      volumes:
          - mongo_data:/data/db
  
  #對應yaml檔案 services名稱
    catalog.api:
      container_name: catalog.api
      environment:
        - ASPNETCORE_ENVIRONMENT=Development
        #連線字串須由localhost變更為Container_name
        #測試結果會變更appsettings內的字串
        - "DatabaseSettings:ConnectionString=mongodb://catalogdb:27017"
        - "ElasticConfiguration:Uri=http://elasticsearch:9200"
      depends_on:
        #必須要在catalogdb container啟動時才能用
        - catalogdb
      ports:
        - "8000:80"
  
  ```

- ```
  
  ```

  

- `docker-compose -f .\docker-compose.yml -f .\docker-compose.override.yml up -d`



## 基本操作

### [Docker Hub](https://hub.docker.com/_/mysql)

- 倉庫是集中存放映像檔檔案的場所。有時候會把倉庫和倉庫註冊伺服器（Registry）混為一談，並不嚴格區分。實際上，倉庫註冊伺服器上往往存放著多個倉庫，每個倉庫中又包含了多個映像檔，每個映像檔有不同的標籤（tag）。
- 倉庫分為公開倉庫（Public）和私有倉庫（Private）兩種形式
- 當使用者建立了自己的映像檔之後就可以使用 `push` 命令將它上傳到公有或者私有倉庫，這樣下次在另外一台機器上使用這個映像檔時候，只需要從倉庫上 `pull` 下來就可以了。
- Docker 倉庫的概念跟 [Git](http://git-scm.com/) 類似，註冊伺服器可以理解為 GitHub 這樣的託管服務。

### Search Image

- ###### `docker search mysql`

### Pull Image

- ###### `docker pull mysql`

### List Image

- ###### `docker images`

### Build Image

- ```powershell
  # ImageName 需小寫
  # 須注意 Dockerfile 的路徑
  docker build -t [ImageName]:[Tag] -f [Dockerfile Name] .
  
  #1. 準備好DockerFile
  #2. 建立image
      # 啟動 Powershell 或 cmd
      # 移至專案 Dockerfile 所在位置的上層資料夾
      # 執行 Docker build 指令, 建立 Image, 參考指令如下
      # -t: Image 名稱及 Tag
      # -f: Dockerfile 檔案路徑
   注意：確保在名稱後面放置一個空格和句點 - 很容易錯過！
   
   docker build -t icw_biosecurity_api:latest -f .\ICW.BioSecurity.Api\Dockerfile .
    
  #OR直接到DockerFile那層
  
  docker build -t icw_biosecurity_api:latest .
  
  ```

### Remove Image

- ```powershell
  docker rmi [ImageID]
  docker image rm [ImageID]
  docker rmi [ImageName]:[Tag]
  
  # remove all images
  docker rmi $(docker images -q) -f
  
  # remove all none images (None Image 是在 build 過程中產生)
  docker rmi $(docker images -q)
  ```

### Run Container

- ```powershell
  # icw 代表Container Name
  # icw_cloud代表Image Name
  # -it: 啟動互動式容器
  # --rm: exit 後刪除 container
  # -p: Port 對應 [外部 IP]:[內部 IP] (可設立多個 mapping)
  # -d: 背景執行
  # docker run  -p  -d [外部 ip]: [內部 ip] --name [容器名稱] [RepositoryName]:[Tag]
  docker run -d -p  8096:80 --name icw icw_cloud
  ```

### List Container

- ```powershell
  docker container ls
  # show all container (includes stopped container)
  docker container ls -all
  docker ps
  ```

### Stop Container

- ```powershell
  docker stop $(docker ps -a -q)
  
  docker stop <CONTAINER ID>
  ```

### Remove Container

- ```powershell
  # remove all stopped container
  docker container prune
  # remove container by container id
  docker container rm [ContainerID]
  # remove ALL
  docker rm $(docker ps -a -q)
  ```






## 資料庫


### MySql 安裝

```powershell
docker search mysql

docker pull mysql
# docker run --name [MySql 名稱] -e MYSQL_ROOT_PASSWORD=my-password -d -p 3306:3306 mysql
# MYSQL_ROOT_PASSWORD = root 的密碼
docker run --name mysql -e MYSQL_ROOT_PASSWORD=root -d -p 3306:3306 mysql
```

### Mongo DB

```powershell
> docker pull mongo

> docker run -d -p 27017:27017 --name shopping-mongo mongo

#安裝Mongo GUI
> docker run -d -p 3000:3000 mongoclient/mongoclient

#透過CLI操作mongo db
> docker exec -it shopping-mongo /bin/bash

> show dbs
admin   0.000GB
config  0.000GB
local   0.000GB
> use CatalogDb
switched to db CatalogDb
> db.createCollection('Products')
{ "ok" : 1 }
> db.Products.insert({"Name":"SSSSS"})
WriteResult({ "nInserted" : 1 })
> db.Products.find().pretty()
{ "_id" : ObjectId("62bedb028d0493cc5f765dd9"), "Name" : "SSSSS" }
```

### Redis

- ```powershell
  > docker pull redis
  
  > docker run -d -p 6379:6379 --name  my-redis redis
  
  #透過CLI操作redis
  > docker exec -it my-redis /bin/bash
  root@e7b360d360b3:/data# redis-cli
  127.0.0.1:6379> ping
  PONG
  127.0.0.1:6379> test
  (error) ERR unknown command 'test', with args beginning with:
  127.0.0.1:6379> set key value
  OK
  127.0.0.1:6379> get key
  "value"
  127.0.0.1:6379> set test AA
  OK
  127.0.0.1:6379> get test
  "AA"
  ```

  

# gRPC

- ##### RPC 的全名是 remote procedure call，主要是作為電腦和電腦間溝通使用。

  - ###### A 電腦可以呼叫 B 電腦執行某些程式，B 電腦會將結果回傳給 A 電腦，A 電腦在收到回應後會再繼續處理其他任務。

  - ###### RPC 的好處在於，雖然 A 電腦是發送請求去請 B 電腦做事，但其呼叫的方式，就很像是 A 電腦直接在呼叫自己內部的函式一般。

- ##### gRPC 也是基於這樣的概念，讓想要呼叫 server 處理請求的 client，在使用這支 API 時就好像是呼叫自己內部的函式一樣簡單自然

  - ###### gRPC 就像 Web 常用的 Restful API 一樣，都是在處理請求和回應，並且進行資料交換，但 gRPC 還多了其他的功能和特色。

- ##### gRPC 是由 Google 開發的開源框架，它快速有效、奠基在 HTTP/2 上提供低延遲（low latency），支援串流，更容易做到權限驗證（authentication）。

- ##### 四種 gRPC 的傳輸方式:

  - ###### 單向傳輸(Unary): Client 單向，Server 單向 客戶端發送一個請求並獲取一個響應。

  - ###### Server 單向串流(Streaming-Server): Client 單向，Server 串流 從客戶端獲取請求後，伺服器將響應流發送回去。

  - ###### Client 單向串流(Streaming-Client): Client 串流，Server 單向 客戶端發送一系列消息，等待服務器對其進行處理並收到單個響應。

  - ###### 雙向串流(Streaming-Bidirectional): Client 串流，Server 串流 客戶端和服務器在兩個方向上交換訊息。

## Protocol Buffers

- ##### 在傳統的 Restful API 中，最常使用的資料交換格式通常是 JSON；但到了 gRPC 中，資料交換的格式則是使用名為 [Protocol Buffers](https://developers.google.com/protocol-buffers/docs/overview) 的規範／語言。

  - ###### 當我們想要使用 gRPC 的服務來交換資料前，必須先把資料「格式」和「方法」都定義清楚。

  - ###### Message: 定義資料結構及屬性型別

  - ![img](https://camo.githubusercontent.com/4eb1a534787b679543e7eb36ab74b5ae0fb48d01bfde6c3d708b60880996f84c/68747470733a2f2f692e696d6775722e636f6d2f6f58324b6e54502e706e67)

- #####  使用 gRPC 前，不只需要先把資料交換的格式定義清楚，同時也需要把資料交換的方法定義清楚。

  - ###### Services: 定義方法

  - ```go
    syntax = "proto3";  // 定義要使用的 protocol buffer 版本
    
    //定義所屬package
    package calculator;  // for name space
    option go_package = "proto/calculator";  // generated code 的 full Go import path
    
    message CalculatorRequest {
      int64 a = 1;
      int64 b = 2;
    }
    
    message CalculatorResponse {
      int64 result = 1;
    }
    
    //定義API interface
    service CalculatorService {
      rpc Sum(CalculatorRequest) returns (CalculatorResponse) {};
    }
    ```

##  gRPC 與 HTTP API 比較

- ###### gRPC 訊息是使用 [Protobuf](https://developers.google.com/protocol-buffers/docs/overview)序列化，這是一種有效率的二進位訊息格式。 Protobuf 會在伺服器和用戶端上非常快速地序列化。 Protobuf 序列化會導致小型訊息承載，在行動裝置應用程式等有限頻寬案例中很重要。

- ###### 文件即是程式結構，程式內不用另外寫object mapping資料

- ###### 文件透過binary直接轉譯成程式，支援多種語言

- ###### 資料會經過protobuf編碼，人類無法直接看懂資料，安全性相較於JSON較高（但同時也是缺點，因為人看不懂）

| 特徵               | gRPC                                                         | 使用 ON 的 JS HTTP API        |
| :----------------- | :----------------------------------------------------------- | :---------------------------- |
| 合約               | 必要 (`.proto`)                                              | 選擇性 (OpenAPI)              |
| 通訊協定           | HTTP/2                                                       | HTTP                          |
| Payload            | [Protobuf (小型二進位)](https://docs.microsoft.com/zh-tw/aspnet/core/grpc/comparison?view=aspnetcore-6.0#performance) | JSON (大型、人類可讀)         |
| 規範性             | [嚴格規格](https://docs.microsoft.com/zh-tw/aspnet/core/grpc/comparison?view=aspnetcore-6.0#strict-specification) | 鬆散。 任何 HTTP 都是有效的。 |
| 串流               | [用戶端、伺服器、雙向](https://docs.microsoft.com/zh-tw/aspnet/core/grpc/comparison?view=aspnetcore-6.0#streaming) | 用戶端、伺服器                |
| 瀏覽器支援         | [不需要 grpc-web) (](https://docs.microsoft.com/zh-tw/aspnet/core/grpc/comparison?view=aspnetcore-6.0#limited-browser-support) | 是                            |
| 安全性             | 傳輸 (TLS)                                                   | 傳輸 (TLS)                    |
| 用戶端程式代碼產生 | [是](https://docs.microsoft.com/zh-tw/aspnet/core/grpc/comparison?view=aspnetcore-6.0#code-generation) | OpenAPI + 協力廠商工具        |

## Net Core 

1. ##### 建立`gRPC` Server專案

   - ![img](https://raw.githubusercontent.com/Sean2416/Pic/master/img/202207021842168.png)

2. 點選專案內`Protos`資料夾，並新增Protoco Buffer File

   - ![img](https://raw.githubusercontent.com/Sean2416/Pic/master/img/202207021844289.png)

3. 編輯檔案內容

   - 建立 `回傳格式`及 `方法`

   - ```
     syntax = "proto3";
     
     option csharp_namespace = "Discount.Grpc.Protos";
     
     service DiscountProtoService {
     
     	rpc GetDiscount (GetDiscountRequest) returns (CouponModel);
     
     	rpc CreateDiscount (CreateDiscountRequest) returns (CouponModel);
     	rpc UpdateDiscount (UpdateDiscountRequest) returns (CouponModel);
     	rpc DeleteDiscount (DeleteDiscountRequest) returns (DeleteDiscountResponse);	  
     }
     
     message GetDiscountRequest {
       string productName = 1;
     }
     
     message CouponModel {
     	int32 id = 1;
     	string productName = 2;
     	string description = 3;
     	int32 amount = 4;
     }
     ```

4. 新增`Service`繼承xxxProtoService的類別並實作內容

   - 步驟3 新增完後，會自動產生`xxxProtoService`類別(ex.`discount.proto`產生DiscountProtoService)

   - 實作所有.proto內定義的方法

   - ```C#
       public class DiscountService : DiscountProtoService.DiscountProtoServiceBase
         {
             private readonly IDiscountRepository _repository;
             private readonly IMapper _mapper;
             private readonly ILogger<DiscountService> _logger;
       
             public DiscountService(IDiscountRepository repository, IMapper mapper, ILogger<DiscountService> logger)
             {
                 _repository = repository ?? throw new ArgumentNullException(nameof(repository));
                 _mapper = mapper ?? throw new ArgumentNullException(nameof(mapper));
                 _logger = logger ?? throw new ArgumentNullException(nameof(logger));
             }
       
             public override async Task<CouponModel> GetDiscount(GetDiscountRequest request, ServerCallContext context)
             {
                 var coupon = await _repository.GetDiscount(request.ProductName);
                 if (coupon == null)
                 {
                     throw new RpcException(new Status(StatusCode.NotFound, $"Discount with ProductName={request.ProductName} is not found."));
                 }
                 var couponModel = _mapper.Map<CouponModel>(coupon);
                 return couponModel;
             }
     ```

5. 在`Startup.cs` > `Configure`內新增Service Mapping

   - ```C#
      public void Configure(IApplicationBuilder app, IWebHostEnvironment env)
             {
                 app.UseEndpoints(endpoints =>
                 {
                     endpoints.MapGrpcService<DiscountService>();
       
                     endpoints.MapGet("/", async context =>
                     {
                         await context.Response.WriteAsync("Communication with gRPC endpoints must be made through a gRPC client. To learn how to create a client, visit: https://go.microsoft.com/fwlink/?linkid=2086909");
                     });
                 });
     ```

6. 使用gRPC in Client

   1. 在要使用的專案新增參考

      - ![img](https://raw.githubusercontent.com/Sean2416/Pic/master/img/202207021854787.png)
      - ![img](https://raw.githubusercontent.com/Sean2416/Pic/master/img/202207021855588.png)
      - ![img](https://raw.githubusercontent.com/Sean2416/Pic/master/img/202207021856221.png)

   2. 以上步驟建立完後，會產生`Protos`資料夾

   3. 在`Startup.cs` > `ConfigureServices`內新增設定

      - ```C#
         public void ConfigureServices(IServiceCollection services)
         {
             services.AddGrpcClient<DiscountProtoService.DiscountProtoServiceClient>
                 (o => o.Address = new Uri(Configuration["GrpcSettings:DiscountUrl"]));
         }
        ```

   4. 用`DiscountProtoService.DiscountProtoServiceClient _discountProtoService` 即可呼叫gRPC服務

      - ```C#
        private readonly DiscountProtoService.DiscountProtoServiceClient _discountProtoService;
        
        public DiscountGrpcService(DiscountProtoService.DiscountProtoServiceClient discountProtoService)
        {
            _discountProtoService = discountProtoService ?? throw new ArgumentNullException(nameof(discountProtoService));
        }
        
        public async Task<CouponModel> GetDiscount(string productName)
        {
            var discountRequest = new GetDiscountRequest { ProductName = productName };
        
            return await _discountProtoService.GetDiscountAsync(discountRequest);
        }
        ```

## 參考

- [BESG gRPC](https://www.youtube.com/watch?v=MYmPY1E17ZM&t=11s)



# Ocelot

##  Api Gateway

- ##### API閘道器是一個伺服器，是系統的唯一入口。API閘道器封裝了系統內部架構，為每個客戶端提供一個定製的API。

- ##### 所有的客戶端和消費端都通過統一的閘道器接入微服務，在閘道器層處理所有的非業務功能。通常，閘道器也是提供REST/HTTP的訪問API。服務端通過API-GW註冊和管理服務。

- ##### API 閘道器是客戶端訪問服務的統一入口，API 閘道器封裝了後端服務，還提供了一些更高階的功能，例如：身份驗證、監控、負載均衡、快取、多協議支援、限流、熔斷等等。

  - ###### 限流：實現微服務訪問流量計算，基於流量計算分析進行限流，可以定義多種限流規則。

  - ###### 快取：資料快取。

  - ###### 日誌：日誌記錄。

  - ###### 監控：記錄請求響應資料，api耗時分析，效能監控。

  - ###### 鑑權：許可權身份認證。

  - ###### 灰度：線上灰度部署，可以減小風險。

  - ###### 路由：路由是API閘道器很核心的模組功能，此模組實現根據請求，鎖定目標微服務並將請求進行轉發。

- ![img](https://images2018.cnblogs.com/blog/381412/201806/381412-20180611222135221-64112379.png)

## Implement

### 實作步驟

1. ##### 建立空白套件

2. ##### Install-Package Ocelot

3. ##### 設定`Program.cs`

   ```C#
   public class Program
   {
       public static void Main(string[] args)
       {
           CreateHostBuilder(args).Build().Run();
       }
   
       public static IHostBuilder CreateHostBuilder(string[] args) =>
           Host.CreateDefaultBuilder(args)
           //對於 Ocelot 而言，此處的重點是您必須透過 AddJsonFile() 方法提供給產生器的 configuration.json 檔案
               .ConfigureAppConfiguration((hostingContext, config) =>
               {
                   config.AddJsonFile($"ocelot.{hostingContext.HostingEnvironment.EnvironmentName}.json", true, true);
               })
               .ConfigureWebHostDefaults(webBuilder =>
               {
                   webBuilder.UseStartup<Startup>();
               });
   }
   ```

4. ##### 設定 `Startup.cs` 

   ```C#
   public void ConfigureServices(IServiceCollection services)
   {
       services.AddOcelot();
   }
   
   // This method gets called by the runtime. Use this method to configure the HTTP request pipeline.
   public async void Configure(IApplicationBuilder app, IWebHostEnvironment env)
   {
       ...
           
       await app.UseOcelot();
   }
   ```

5. ##### 建立設定檔案

### Ocelot .Json

- ![.NET Core 微服務—API閘道器(Ocelot) 教程 [一]](https://i.iter01.com/images/91882e5808b330a088525cb2ff2d4c99d8b471d70dc895f9c6c079277aa86ad3.png)

#### 基本架構

- ###### Routes 是告知 Ocelot 如何處理上游要求的物件

- ###### Ocelot API 閘道的主要功能是接受傳入 HTTP 要求並將其轉送到下游服務 

- ###### BaseUrl定義對外的URL

- ###### Route

  - 當Client發送一個Request`GET https://api.mybusiness.com/Catalog`

  - ###### ApiGetway根據設定內容，找到Upstream設定為/Catalog 且方法為Get的設定檔

  - ###### 導引至對應的URL `http://localhost:8000/api/v1/Catalog`

  - ![img](https://raw.githubusercontent.com/Sean2416/Pic/master/img/202207092303548.png)

- ```C#
  {
      "Routes": [
          //Catalog API
          {
            //Downstream 對應內部服務設定
            "DownstreamPathTemplate": "/api/v1/Catalog",
            "DownstreamScheme": "http",
            "DownstreamHostAndPorts": [
              {
                "Host": "localhost",
                "Port": "8000"
              }
            ],
            //Upstream對應Client服務設定
            "UpstreamPathTemplate": "/Catalog",
            "UpstreamHttpMethod": [ "GET", "POST", "PUT" ],
            "FileCacheOptions": { "TtlSeconds": 30 }
          },
          {
            "DownstreamPathTemplate": "/api/v1/Basket/Checkout",
            "DownstreamScheme": "http",
            "DownstreamHostAndPorts": [
              {
                "Host": "localhost",
                "Port": "8001"
              }
            ],
            "UpstreamPathTemplate": "/Basket/Checkout",
            "UpstreamHttpMethod": [ "POST" ],
            "RateLimitOptions": {
              "ClientWhitelist": [],
              "EnableRateLimiting": true,
              "Period": "3s",
              "PeriodTimespan": 1,
              "Limit": 1
            }
          }
      ],
      "GlobalConfiguration": {
          "BaseUrl": "https://api.mybusiness.com"
      }
  }
  ```

#### 區分大小寫

- ###### 預設不區分

- ###### 想要過濾大小寫的話可以在設定檔加入`"RouteIsCaseSensitive": true`

#### 流量限制- RateLimitOptions

- `ClientWhitelist` - 設定Client白名單清單，在清單內的Client不會受到流量限制。

- `EnableRateLimiting` - 是否開啟流量限制。

- `Period` - 指定流量限制時間，例如1s，5m，1h，1d等,

- `PeriodTimespan` - 恢复等待时间，當訪問次數超過限制數時，需等待多長時間才能再次訪問

- `Limit` - 指定Client在時間(`Period` )內允許的最大請求數

- ##### 下方設定檔案說明

  - ##### 指定Client在1分鐘之內僅能發起一次請求，超過則block request

    - ![img](https://raw.githubusercontent.com/Sean2416/Pic/master/img/202207101016301.png)
    - ![img](https://raw.githubusercontent.com/Sean2416/Pic/master/img/202207101008111.png)

  - ##### 在block 30後秒才能再次發送請求

- ```C#
  {
    "DownstreamPathTemplate": "/api/v1/Catalog",
    "DownstreamScheme": "http",
    "DownstreamHostAndPorts": [
      {
        "Host": "localhost",
        "Port": "8000"
      }
    ],
    "UpstreamPathTemplate": "/Catalog",
    "UpstreamHttpMethod": [ "GET", "POST", "PUT" ],
    "RateLimitOptions": {
      "ClientWhitelist": [],
      "EnableRateLimiting": true,
      "Period": "1m",
      "PeriodTimespan": 30,
      "Limit": 1
    }
  ```

#### Cache

- 加入方法

  - 下載`Install-Package Ocelot.Cache.CacheManager`

  - 調整`ConfigureServices`

    - ```C#
      s.AddOcelot()
          .AddCacheManager(x =>
          {
              x.WithDictionaryHandle();
          })
      ```

  - ##### 加入設定檔

    - ```C#
      "FileCacheOptions": {
        "TtlSeconds": 300, // Cache保留時間
        "Region": "somename" //自定義區域名稱,代表Cache要配置到哪個暫存區
      }
      ```

- 使用結果

  - ###### 可以看到Ocelot 在第一次取得Catalog資料後就存入Cache中(由於Cache保留時間為五分鐘)，因此後面雖然更新了`62bf05b24f0daf9c82e4810d`的資料內容

    - ![img](https://raw.githubusercontent.com/Sean2416/Pic/master/img/202207101041771.png)

  - ###### 但是透過Ocelot取得的資料依舊是舊的，必須等到Cache被清除後資料才會回去本地端服務蟲抓

    - ![img](https://raw.githubusercontent.com/Sean2416/Pic/master/img/202207101040433.png)

  - ##### 不透過Ocelot，直接呼叫API看資料確定是更新過

    - ![img](https://raw.githubusercontent.com/Sean2416/Pic/master/img/202207101043563.png)

#### Request Aggregation

- ##### 將多個request聚合成一個response

- ##### 配置步驟

  1. ######  給定每一個要聚合的request 唯一key值

  2. ######  新增聚合設定`Aggregates`

     - ```C#
       {
           "DownstreamPathTemplate": "/api/v1/Catalog",
           "DownstreamScheme": "http",
           "DownstreamHostAndPorts": [
             {
               "Host": "localhost",
               "Port": "8000"
             }
           ],
           "Key": "Catalog", //給定每一個要聚合的request 唯一key值
           "UpstreamPathTemplate": "/Catalog",
           "UpstreamHttpMethod": [ "GET", "POST", "PUT" ],
           "FileCacheOptions": {
             "TtlSeconds": 30,
             "Region": "somename"
           }
         },
         {
           "DownstreamPathTemplate": "/api/v1/Discount/{Name}",
           "DownstreamScheme": "http",
           "DownstreamHostAndPorts": [
             {
               "Host": "localhost",
               "Port": "8002"
             }
           ],
           "Key": "Discount",
           "UpstreamPathTemplate": "/Discount/{Name}",
           "UpstreamHttpMethod": [ "GET", "DELETE" ]
         }
       ],
       "Aggregates": [
         {
           "RouteKeys": [ //對應步驟1建立的key清單
             "Catalog",
             "Discount"
           ],
           "UpstreamPathTemplate": "/GetDetail/{Name}" //開放給Client呼叫的API端
         }
       ],
       "GlobalConfiguration": {
         "BaseUrl": "http://localhost:5010"
       }
       ```

  3. ##### 如果要針對各聚合做處理，可以建立繼承`IDefinedAggregator`的類別

     1. ###### 建立繼承`IDefinedAggregator`的類別，並實作Aggregate

        ```C#
        public class MyAggregator : IDefinedAggregator
        {
            public async Task<DownstreamResponse> Aggregate(List<HttpContext> responses)
            {
                var one = await responses[0].Items.DownstreamResponse().Content.ReadAsStringAsync();
                var two = await responses[1].Items.DownstreamResponse().Content.ReadAsStringAsync();
        
                var contentBuilder = new StringBuilder();
                contentBuilder.Append(one);
                contentBuilder.Append(two);
        
                var stringContent = new StringContent(contentBuilder.ToString())
                {
                    Headers = { ContentType = new MediaTypeHeaderValue("application/json") }
                };
        
                return new DownstreamResponse(stringContent, HttpStatusCode.OK, new List<KeyValuePair<string, IEnumerable<string>>>(), "OK");
            }
        }
        ```

     2. ###### 於ConfigureServices進行註冊

        ```C#
        public void ConfigureServices(IServiceCollection services)
        {
            services.AddOcelot().AddCacheManager(x =>
            {
                x.WithDictionaryHandle();
            })
            .AddSingletonDefinedAggregator<MyAggregator>();
        }
        ```

     3. ###### 新增至設定檔

        ```C#
        {
        	"Aggregates": [
                {
                  "RouteKeys": [
                    "User",
                    "Product"
                  ],
                  "UpstreamPathTemplate": "/UserAndProduct",
                  "Aggregator": "MyAggregator"
                }
              ],
        }
        ```

#### 啟動所有Route

- ###### 这个配置将会把路径+查询字符串统统轉發到本地端.

- ###### URL只是變數名稱，可隨時替換

  - ###### 這種配置方式優先權最低，如果request在設定檔中有對應的設定會優先配對

- ```C#
  {
      "DownstreamPathTemplate": "/{url}",
      "DownstreamScheme": "http",
      "DownstreamHostAndPorts": [
              {
                  "Host": "localhost",
                  "Port": 1001,
              }
          ],
      "UpstreamPathTemplate": "/{url}",
      "UpstreamHttpMethod": [ "Get" ]
  }
  ```

#### 優先權設定

- ###### 在route裡面可以設定Priority指定配對全縣

- ###### 0 是最低優先級

- ```C#
  {
      "UpstreamPathTemplate": "/goods/{catchAll}"
      "Priority": 0
  }
  
  {
      "UpstreamPathTemplate": "/goods/delete"
      "Priority": 1
  }
  //上面两个路由中，如果向Ocelot发出的请求时/goods/delete格式的话，则Ocelot会优先匹配/goods/delete 的路由。
  ```

#### Docker設定

- ##### 在Docker中，服務之間可以透過`ContainerName`進行呼叫而不用指定URL(就像在YML檔裡面設定連接資料庫也是直接使用Container Name同理)

  - ![img](https://raw.githubusercontent.com/Sean2416/Pic/master/img/202207101100714.png)

- ##### 因此，JsonFile內的`DownstreamHostAndPorts`也可以直接使用Container做設定

  - ```C#
    {
      "DownstreamPathTemplate": "/api/v1/Catalog",
      "DownstreamScheme": "http",
      "DownstreamHostAndPorts": [
        {
          "Host": "catalog.api", //ContainerName
          "Port": "80" //Port 為Image設定的內部Port，並非Container對外的Port
        }
      ],
      "UpstreamPathTemplate": "/Catalog",
      "UpstreamHttpMethod": [ "GET", "POST", "PUT" ],
      "FileCacheOptions": { "TtlSeconds": 30 }
    },
    ```

    

## 參考

- [Ocelot_實用技巧 - 程式人生](https://www.796t.com/article.php?id=123886)
- [Ocelot簡易教程](https://github.com/yilezhu/OcelotDemo/wiki)
- [Getting Started — 官網](https://ocelot.readthedocs.io/en/latest/introduction/gettingstarted.html)
- [NET Core 微服務—Request Aggregation](https://codingnote.cc/zh-tw/p/172313/)



# Kubernetes 

## Quick start

1. #####  安裝K8S及Docker

   - 確認本機開啟Hyperv
   - 確認本機模擬開啟
     - 開啟方式: 進入Bios >  CPU選項> 開啟虛擬化
     - ![img](https://raw.githubusercontent.com/Sean2416/Pic/master/img/202206190955608.png)
   - 安裝minikube 、 kubectl 套件

2. ##### 製作Docker Container

   - 實作Docker Image並推行上Register(此處使用[Docker hub](https://hub.docker.com/)， Docker 必須啟動)

   - ```powershell
     > docker login -u sean2416 -p A128277902
     
     > docker tag icw_cloud sean2416/docker-demo:latest
     
     > docker push sean2416/docker-demo:latest
     ```

3. ##### 啟動 Minikube 

   - `minikube start --vm-driver=hyperv`

4. ##### Minikube 上跑起你的 Docker Containers 

   - 透過Container 建立 K8S pod

   - 撰寫yaml檔案

     - ```yaml
       # my-first-pod.yaml
       apiVersion: v1
       kind: Pod
       metadata:
         name: my-pod
         labels:
           app: webserver
       spec:
         containers:
         - name: pod-demo
           image: sean2416/docker-demo
           ports:
           - containerPort: 80
       ```

   - 建立Pod

     - ```yaml
        kubectl create -f my-first-pod1.yaml
       ```

   - 查看Pod

     - ```
       kubectl get pods 
       ```

5. ##### 與 Pod 中的 container 互動

   1. 透過kubectl port-forward

      - 將Container所在的 80 port 對應到外部8000(內部Port參考Image建置時所指定的Port號)
      - `kubectl port-forward my-pod 8000:80`
      - ![img](https://raw.githubusercontent.com/Sean2416/Pic/master/img/202206191010205.png)

   2.  建立一個 Service

      - **kubectl port-forward 是將 pod 的 port mapping 到本機端上，而 kubectl expose 則是將 pod 的 port 與 Kubernetes Cluster 的 port number 做 mapping。**

      -  `kubectl expose` 指令，創建 `my-pod-service` 的 Service 物件

        - ```powershell
          > kubectl expose pod my-pod --type=NodePort --name=my-pod-service
          service/my-pod-service exposed
          ```

      - 查看Service狀態

        - ```powershell
          > kubectl get services
          NAME             TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE
          kubernetes       ClusterIP   10.96.0.1        <none>        443/TCP        36h
          my-pod-service   NodePort    10.107.219.187   <none>        80:32446/TCP   2s
          ```

      - 取得service URL

        - ```
          > minikube service my-pod-service --url
          http://172.31.143.191:30447
          ```

6. ##### 建立Deployment

   1. 建立yaml檔案

      - ```yaml
        apiVersion: apps/v1
        kind: Deployment
        metadata:
          name: hello-deployment
        spec:
          replicas: 3
          selector:
            matchLabels:
              app: my-deployment
          template:
            metadata:
              labels:
                app: my-deployment
            #在一個Pod中 產生兩個Container
            spec:
              containers:
              - name: my-pod
                image: zxcvbnius/docker-demo:latest
                ports:
                - containerPort: 3000        
              - name: pod-demo
                image: sean2416/docker-demo
                ports:
                - containerPort: 80
        ```

   2. 新增Deployment `kubectl create -f ./my-deployment.yaml`

   3. 查看Delplyment ` kubectl get deployments`

7. ##### 建立Service

   - 建立yaml檔案

     - ```yaml
       # service-example.yaml
       apiVersion: v1
       kind: Service
       metadata:
         name: hello-service
       spec:
         type: NodePort
         ports:
           - name: http-test1
             port: 80  #對應Container Port
             protocol: TCP
             nodePort: 30390
           - name: http-test2
             port: 3000
             protocol: TCP
             nodePort: 30391
         selector:
           app: my-deployment
       ```

   - ##### 產生Service

     - ```
        kubectl create -f ./my-service.yaml
       ```

     

- ###### 參考文章

  - [How to install Kubernetes on windows 10](https://github.com/twtrubiks/k8s-tutorial/tree/master/How_to_install_k8s_on_win10)
  - [Minikube 安裝與配置](https://ithelp.ithome.com.tw/articles/10192490)
  - [Kubernetes - minikube (輕鬆建立本地端的 K8S 集群工具) 安裝教學](https://blog.kennycoder.io/2020/03/15/Kubernetes-minikube-%E8%BC%95%E9%AC%86%E5%BB%BA%E7%AB%8B%E6%9C%AC%E5%9C%B0%E7%AB%AF%E7%9A%84K8S%E9%9B%86%E7%BE%A4%E5%B7%A5%E5%85%B7-%E5%AE%89%E8%A3%9D%E6%95%99%E5%AD%B8/)



## 基本介紹

### 主要架構

​	

![img](https://raw.githubusercontent.com/Sean2416/Pic/master/img/202206152030525.png)

![https://ithelp.ithome.com.tw/upload/images/20201001/20129737wF5PKYvbLN.png](https://ithelp.ithome.com.tw/upload/images/20201001/20129737wF5PKYvbLN.png)

#### Master Node

- ##### Master Node主要用來管理Work Node(也就是Slave Node, Kubernetes Node)，進行工作的調配與規劃..等

  - ###### 取得master node components的狀態 `kubectl get componentstatuses`

  - ###### Get components detail `kubectl describe componentstatuses controller-manager`

##### ETCD

- ###### 每個Cluster都會有著一個以上的etcd，etcd用一致且高可用的鍵值方式儲存Kubernetes叢集的資料，Kubernetes Cluster會預設將etcd資料作為備份

##### API Server

- ###### 透過API-Server對外曝露所有的Kubernetes API，亦可以把它當作是Kubernetes Control Plane的前端。而我們操作的Kubernetes ctl也是我們透過kubectl的方式與API-Server進行溝通

- ###### API-Server有著以下幾個功能：

  - ###### 提供API讓使用者能夠取得叢集內部各資源資訊、創建/更改/移除 各資源或下達調度策略對各資源進行調配。

  - ###### 代理群集當中一些額外組件，像是Kubernetes UI、metrics-server..etc

  - ###### 創建Kubernetes server

  - ###### 讓資源進行版本更新

- ###### 通過`kubectl proxy`得知目前叢集最上層的api path為何

##### Kube-Controller Manager

- ###### 負責管理並運行 Kubernetes controller 的組件，簡單來說 controller 就是 Kubernetes 裡一個個負責監視 Cluster 狀態的 Process，例如：Node Controller、Replication Controller

- ###### 這些 Process 會在 Cluster 與預期狀態（desire state）不符時嘗試更新現有狀態（current state）。例如：現在要多開一台機器以應付突然增加的流量，那我的預期狀態就會更新成 N+1，現有狀態為 N，這時相對應的 controller 就會想辦法多開一台機器

- ###### controller-manager 的監視與嘗試更新也都需要透過訪問 kube-apiserver 達成

  - ###### Node controller: 負責當Node出現故障時的訊息通知與增減

  - ###### Replication controller: 負責管理Pods數量與狀態，但現在使用ReplicaSet居多

  - ###### Endpoints controller: 負責生成與維護所有endppoints物件(such as service)

  - ###### Service Account & Token controllers: 負責管理用戶帳戶與服務

##### Scheduler

- ###### 整個 Kubernetes 的 Pods 調度員，scheduler 會監視新建立但還沒有被指定要跑在哪個 Node 上的 Pod，並根據每個 Node 上面資源規定、硬體限制等條件去協調出一個最適合放置的 Node 讓該 Pod 跑

- ###### 依據機器資源、軟體資源(叢集)、調度決策、affinity and anti-affinity親和力與反親和力等多方考量，去決定Pods是否新建與Pods的數量分配(分配至Ｗorker Node)。因為每個Pods都會有自己的requirements(像是平均Memory用量超過50%新建、不建立該種Pod在Node-1上..等)，所以在叢集調度上並非易事，也因此有了該components的產生

- ###### affinity又有分成是Node affinity與 Pod affinity，affinity也是屬於種調度策略

##### 基本建置流程

- ![img](https://miro.medium.com/max/1050/0*5N7SlevIHOdKB-yC)

- ###### 上圖為一個簡易的 Kubernetes Cluster，通常一個 Cluster 中其實會有多個 Master 作為備援，但為了簡化我們只顯示一個

- ###### 當使用者要部署一個新的 Pod 到 Kubernetes Cluster 時，使用者要先透過 User Command（kubectl）輸入建立 Pod 的對應指令（下面會在解說如何實際動手操作來建立一個 Pod）。

- ###### 此時指令會經過一層確認使用者身份的認證後，傳遞到 Master Node 中的 API Server，API Server 會把指令備份到 etcd

- ###### 接下來 controller-manager 會從 API Server 收到需要創建一個新的 Pod 的訊息，並檢查如果資源許可，就會建立一個新的 Pod。

- ###### 最後 Scheduler 在定期訪問 API Server 時，會詢問 controller-manager 是否有建置新的 Pod，如果發現新建立的 Pod 時，Scheduler 就會負責把 Pod 配送到最適合的一個 Node 上面。

#### Kubernetes  Node

- ##### Worker node是用來部署容器的地方，也就是運行服務的機器，所以每個Node中必備著能夠建置容器的執行環境，像是Docker等。

  - ###### Get all nodes `kubectl get nodes`

  - ###### Describe specific node `kubectl describe nodes <node_name>`

  - ##### Addresses

    - ###### HostName: 該節點的host name，可以透過kubectl —hostname-override 來覆寫他。

    - ###### ExternalIP: 該節點可路由的外部IP，提供群集外部使用。

    - ###### InternalIP: 該節點可路由的內部IP，僅叢集內部能夠路由。

  - ##### Conditions

    - 描述所有運行節點目前的狀態，狀態的描述有以下幾種：

    - |   Node Condition   |                         Description                          |
      | :----------------: | :----------------------------------------------------------: |
      |       Ready        | True表示節點運行狀況良好並準備好接受Pod，False表示節點運行狀況不佳並且不接受Pod，Unknown表示節點控制器最近一次未從節點收到消息node-monitor-grace-period（默認值為40秒） |
      |    DiskPressure    |             True表示磁盤容量不足；除此以外False              |
      |   MemoryPressure   |             True表示節點內存不足; 除此以外False              |
      |    PIDPressure     |          True表示節點上的Process太多；除此以外False          |
      | NetworkUnavailable |           True表示節點的網絡配置不正確，否則 False           |

  - ##### Capacity and Allocatable

    - ###### 描述該節點上可用資源最大數量，包含cpu、memory與pods的數量

  - ##### System Info

    - ###### 該節點上各種軟硬體設備的訊息，包含uuid與版本號....等

##### Kubelet

- ###### Kubernetes是一個分散式的集群管理系統，在每個worker 上運行一個worker process對node上的Container做周期性管理，而這個worker就是Kubelet。

- ##### 主要功能

  - ###### Pod的管理: 如上述，一個pod由一或多個containers組成，彼此共享pod中的資源與port，所以同個pod間能透過localhost進行溝通，因此也可利用volume與mount將資源共享至多個容器當中，kubelet就是負責管理這些pod資源

  - ###### 健康檢查: 創建容器後，如果想確認容器是否正常啟動，可以加入health check在pod/deployment的yaml當中，再啟動pod時kubelet會去執行yaml中的health check，只要health check沒過kubelet就會刪除該pod並依照重啟策略處理(預設為刪除後不進行重啟)

  - ###### 容器監測: 透過建置cAdvisor進行監測。

##### CAdvisor

- ###### cAdvisor是一個worker，並即時性的對該Node上所有的資源與容器進行監測與數據的採集，像是CPU、Memory的用量、網路的流量與Storage的使用量等。cAdvisor集成於Kubelet當中，當使用Kubelet時會自動地啟動cAdvisor。

##### Proxy

- ![img](https://ithelp.ithome.com.tw/upload/images/20201002/20129737lEny7hhLuh.png)

- ###### 每個pod都會有個ip，但pod是經常在發生變化的，每次更新ip位置都會有變。為此kubernetes有個component叫做**service**，每個service都會有一組**固定的虛擬ip(clusterIp)**，並且自動地綁定某種類型的pod，有點類似某種pod的專用通道，所有對於該類型pod的request都會透過service進行load balance與redirect。為了實現該功能，在每個Node上都會有個Kube-Proxy，得以當作service, api-server與pod間溝通的橋樑。

##### POD

- ![img](https://miro.medium.com/max/872/1*Hvc0M9UutuTBNQoMRHMkAw.png)

- ##### Pod是在kubernetes當中，能夠創建與運行的最小執行單位，在Pod當中能夠有著一個或多個Containers，並且這些Containers共享著Pod的資源

  - ##### Pod有以下特點

    - ###### 每個 Pod 都有屬於自己的 [yaml](https://zh.wikipedia.org/wiki/YAML) 檔

    - ###### 一個 Pod 裡面可以包含一個或多個 Docker Container

    - ###### 在同一個 Pod 裡面的 containers，可以用 **local port numbers** 來互相溝通

- ##### yaml file

  - ```powershell
    # my-first-pod.yaml
    apiVersion: v1
    kind: Pod
    metadata:
      name: my-pod
      labels:
        app: webserver
    spec:
      containers:
      - name: pod-demo
        image: sean2416/docker-demo
        ports:
        - containerPort: 80 
        #注意可能要跟著Image裡面的expose port設定(待確認)
    ```

  - **apiVersion**

    - 代表目前 Kubernetes 中該元件的版本號。以上述的例子 `Pod` 是 `v1`，而 `v1也是目前Kubernetes中核心的版本號`

  - **metadata**

    - **name** - 指定名稱
    - **labels** - labels` 是 Kubernetes 的是核心的元件之一，Kubernetes 會透過 `Label Selector` 將Pod分群管理。
    - **annotations** - annotations 的功能與 labels 相似。相較於labels，`annotations 通常是使用者任意自定義的附加資訊`，提供外部進行查詢使用，像是版本號，發布日期等等。

  - ##### **spec**

    - 定義 container，在這個範例中，一個 Pod 只運行一個 container。
    - **name** - 我們可以在這 container 中設定 container 的名稱
    - **image** - Image 則是根據 [Docker Registry](https://docs.docker.com/registry/) 提供的可下載路徑。
    - **container.ports** - 以指定` container 有哪些 port number 是允許外部資源存取`，而在這裡我們只允許container中的port 3000對外開放。

  - ##### 建立Pod

    - `kubectl create -f my-first-pod.yaml`

  - 查看狀態

    - `kubectl get pods `

  - 取得更多描述

    - `kubectl describe pods my-pod`

##### Replication Controller

- ###### Kubernetes上用來管理Pod的數量以及狀態的controller

- ###### 每個Replication Controller都有屬於自己的 [yaml](https://zh.wikipedia.org/wiki/YAML) 檔

- ###### Replication Controller設定檔中`可以指定同時有多少個相同的Pods`運行在Kubernetes Cluster上

- ###### 當某一Pod發生crash, failed，而終止運行時，Replication Controller會幫我們自動偵測，並且自動創建一個新的Pod，確保`Pod運行的數量與設定檔的指定的數量相同`

- ###### 當機器重新開啟時，之前在機器上運行的 Replication Controller 會自動被建立，確保pod隨時都在運行。

- ###### yaml file

  - ```yaml
    apiVersion: v1
    kind: ReplicationController
    metadata:
      name: my-replication-controller
    spec:
      replicas: 3
      selector:
        app: hello-pod-v1
      template:
        metadata:
          labels:
            app: hello-pod-v1
        spec:
          containers:
          - name: my-pod
            image: zxcvbnius/docker-demo
            ports:
            - containerPort: 3000
    ```

  - **apiVersion**

    - 代表目前 Kubernetes 中該元件的版本號。以上述的例子 `Pod` 是 `v1`，而 `v1也是目前Kubernetes中核心的版本號`

  - **metadata**

    - **name** - 指定名稱
    - **labels** - labels` 是 Kubernetes 的是核心的元件之一，Kubernetes 會透過 `Label Selector` 將Pod分群管理。
    - **annotations** - annotations 的功能與 labels 相似。相較於labels，`annotations 通常是使用者任意自定義的附加資訊`，提供外部進行查詢使用，像是版本號，發布日期等等。
    - **replicas & selector**
      - 在`replicas`中，我們必須定義`Pod的數量`，
      - 在`spec.selector`中指定我們要選擇的Pod的條件(labels)。
    - **template** 
      - 定義pod的資訊，包含Pod的labels以及Pod中要運行的container。
    - **template.metadata**
      則是Pod的labels，metadata.labels必須被包含在`select`中，否則在創建Replication Controller物件時，會發生error。
    - **template.spec**
      - 定義container，可以參考 [Pod的yaml檔](https://github.com/zxcvbnius/k8s-30-day-sharing/blob/master/Day05/demo-pod/my-first-pod.yaml) 的範例，在我們的範例中，一個Pod只有一個container。

  - ###### 透過 kubectl 操作 Replication Controller 物件

    - 透過`kubectl create`的指令，創建一個新的Replication Controller

      - ![kubectl-create-rc](https://github.com/zxcvbnius/k8s-30-day-sharing/blob/master/Day07/kubectl-create-rc.png?raw=true)

    - 使用`kubectl get rc`查看目前狀態

      - 從圖中可以知道，`my-replication-controller`這個物件目前管理3個Pod，且3個Pod的狀態皆為`Ready`
      - ![kubectl-get-rc](https://github.com/zxcvbnius/k8s-30-day-sharing/blob/master/Day07/kubectl-get-rc.png?raw=true)

    - 這時我們手動刪除其中一個Pod，我們來看看會發生什麼事情

      - ```
        $ kubectl delete pod my-replication-controller-4ftnj
        pod "my-replication-controller-4ftnj" deleted
        ```

      - ![kubectl-rc-recreate-new-pod](https://github.com/zxcvbnius/k8s-30-day-sharing/blob/master/Day07/kubectl-rc-recreate-new-pod-1.png?raw=true)

      - replication controller偵測到一個Pod終止服務時，產生另外一個新的Pod，來確保Pod的數量。

    - 透過`kubectl scale`來scaling Pod的數量

      - ```C#
        $ kubectl scale --replicas=4 -f ./my-replication-controller.yaml
        replicationcontroller "my-replication-controller" scaled
        ```

    - `刪除replication controller`時，要特別注意，由replication controller產生的`pod`也會因此而終止服務。

      - ![kubectl-delete-rc](https://github.com/zxcvbnius/k8s-30-day-sharing/blob/master/Day07/kubectl-delete-rc.png?raw=true)

    - 如果你希望刪掉replication controller之後，這些Pod仍然運行，可以指定 `--cascade=false`，指令如下：

      - `$ kubectl delete rc my-replication-controller *--cascade=false*`

##### Deployment

- ##### k8s在V1.2版本開始，引入了deployment控制器，值得一提的是，這種控制器並不直接管理pod而是通過管理replicaset來間接管理pod。

  - ![k8s之deployment詳解](https://i.iter01.com/images/bd6bad13b9e1ea370d4288231c991c6eb6667c922f8700badd9c189b258c4b5f.png)

- ##### [Deployment](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/) 可以幫我們達成以下幾件事情

  - ###### 支援replicaset的所有功能

  - 支援釋出的停止、繼續

  - 支援版本的滾動更新和版本回退

- ##### 自我解讀

  - Deployment 可以透過yaml設定檔達到服務快速佈署
  - 可以做到快速升級及還原

- ##### Yaml

  - ```yaml
    apiVersion: apps/v1beta2 # for kubectl versions >= 1.9.0 use apps/v1
    kind: Deployment
    metadata:
      name: hello-deployment
    spec:
      replicas: 3
      # 同時建立 3 個 my-deployment 的 pod
      # replicaset 的效果套用在帶有 app=my-deployment 的 pod 上
      # 必須要與下面的 pod label 有相符合
      selector:
        matchLabels:
          app: my-deployment
       # .spec.template 其實就是 pod 的定義
      template:
        metadata:
          # 設定給 pod 的 label 資訊
          labels:
            app: my-deployment
        spec:
          containers:
          - name: my-pod
            image: zxcvbnius/docker-demo:latest
            ports:
            - containerPort: 3000
    ```

  - ###### apiVersion

    - **kubectl**的版本 >= 1.9，則需使用`app/v1`；如果版本號是在1.9以前的話，則需使用`apps/v1beta2`，可以用`kubectl version`查看目前的版本號

  - 使用`kubectl create`指令來新建一個 Deployment 物件

    - ```
      $ kubectl create -f ./my-deployment.yaml
      deployment "hello-deployment" created
      ```

  - 用`kubectl get`查看deployment與Pod的狀態

    -  	![img](https://github.com/zxcvbnius/k8s-30-day-sharing/blob/master/Day08/kubectl-get-deployment.png?raw=true)
    -  	可以發現 Deployment已自動幫我們創建Pod，且這個Pod都帶有**app=my-deployment**的label，而在同時，Deployment也會自動幫我們建立一個`Replication Set`來管理這些Pod
        -  	READY  就緒幾個/總共幾個 
        -  	UP-TO-DATE 有幾個 pod 副本已經 onboard 
        -  	AVAILABLE 目前有多少pod 副本可以運作 
        -  	Age pod運行時間

  - ###### 升級Pod中的Image

    - 透過 set image 

      - ```powershell
        #透過 set image 升級 hello-deployment 裡面的my-pod 指向新的Container
        > kubectl set image deploy/hello-deployment my-pod=zxcvbnius/docker-demo:v2.0.0
        deployment.apps/hello-deployment image updated
        ```

    - kubectl edit 編輯yaml檔案

      - ```powershell
        > kubectl edit deploy hello-deployment
        deployment.apps/hello-deployment edited
        ```

  - ###### 查看升級狀態

    - ```powershell
      > kubectl rollout status deploy hello-deployment
      Waiting for deployment "hello-deployment" rollout to finish: 2 out of 3 new replicas have been updated...
      ```

  - ###### 查看版本紀錄

    - ```powershell
      > kubectl rollout history deploy hello-deployment
      deployment.apps/hello-deployment
      REVISION  CHANGE-CAUSE
      1         <none>
      7         <none>
      10        <none>
      11        <none>
      ```

  - ###### Rolled back

    - 回溯至前一版本

      - ```powershell
        > kubectl rollout undo deployment hello-deployment
        deployment.apps/hello-deployment rolled back
        ```

    - 回至特定版本

      - ```
        > kubectl rollout undo deploy hello-deployment --to-revision=3
        ```

  - ##### Horizontal Pod Autoscaler

    - ```powershell
      > kubectl  autoscale deployment hello-deployment --min=5 --max=15 --cpu-percent=70
      horizontalpodautoscaler.autoscaling/hello-deployment autoscaled
      ## 上面意思為，我最小需求 POD 為 5 個，最大不能超過 15 個，只要 POD CPU使用率超過 70% 就做擴展。
      ## 所以他啟動時會從 5 個，依照 loading 慢慢往上加。
      
      ## 這邊可以看到產生5個Pods
      > kubectl get pods
      NAME                                READY   STATUS    RESTARTS   AGE
      hello-deployment-58b796b8b5-6mdpg   2/2     Running   0          40s
      hello-deployment-58b796b8b5-6w5bn   2/2     Running   0          66m
      hello-deployment-58b796b8b5-85ztt   2/2     Running   0          54m
      hello-deployment-58b796b8b5-r5cs8   2/2     Running   0          65m
      hello-deployment-58b796b8b5-vfq59   2/2     Running   0          40s
      hello-pod                           1/1     Running   0          91m
      my-pod                              1/1     Running   0          89m
      
      PS > kubectl delete pods hello-deployment-58b796b8b5-6mdpg
      pod "hello-deployment-58b796b8b5-6mdpg" deleted
      
      ## 當其中一個Pod被關閉後，會自動建立新的Pod補足最小五個的規則
      PS D:\IIS\Test> kubectl get pods
      NAME                                READY   STATUS        RESTARTS   AGE
      hello-deployment-58b796b8b5-6mdpg   2/2     Terminating   0          92s
      hello-deployment-58b796b8b5-6w5bn   2/2     Running       0          67m
      hello-deployment-58b796b8b5-85ztt   2/2     Running       0          55m
      hello-deployment-58b796b8b5-m92fh   2/2     Running       0          27s
      hello-deployment-58b796b8b5-r5cs8   2/2     Running       0          66m
      hello-deployment-58b796b8b5-vfq59   2/2     Running       0          92s
      hello-pod                           1/1     Running       0          91m
      my-pod                              1/1     Running       0          90m
      ```

    - 查看HPA 的設定

      - ```powershell
        > kubectl  get hpa
        NAME               REFERENCE                     TARGETS         MINPODS   MAXPODS   REPLICAS   AGE
        hello-deployment   Deployment/hello-deployment   <unknown>/70%   5         15        5          7m55s
        ```

  - ##### RollingUpdateStrategy

    - ###### 決定你在更新當下，可以維持多少 POD 是正常運作的。

    - ###### 如果不想要 RollingUpdate 可以強制把 strategy.type 設定成 `Recreate` (預設是 `RollingUpdate`)，這樣升級策略就會變成，他會一次把全部 POD 關閉，然後再一次把全部 POD 升級上來。這樣服務就無法做到無中斷更新。

    - ```yaml
      ...
        strategy:
          type: RollingUpdate
          rollingUpdate:
            # 升級過程中最多可以比原先設定所多出的 pod 數量 ex 下面是指升級過程中 POD 總數可能會變到 4 個(上面設定 replicas 為 3)
            maxSurge: 1
            # 最多可以有幾個 pod 處在無法服務的狀態 ex 下面為最多一個 POD 為不可服務狀態。
            maxUnavailable: 1
        # 容器內應用程式的啟動時間，Kubernetes 會等待設定的時間後才繼續進行升級流程 (沒設定 POD 完成會直接啟動）
        minReadySeconds: 5
        template:
          metadata:
            labels:
            
            ...
      ```

      

- ##### 常見的指令

  - |                      Deployment相關指令                      |                  指令功能                   |
    | :----------------------------------------------------------: | :-----------------------------------------: |
    |                   kubectl get deployments                    |   取得目前Kubernetes中的deployments的資訊   |
    |                        kubectl get rs                        | 取得目前Kubernetes中的Replication Set的資訊 |
    |          kubectl describe deploy <deployment-name>           |        取得特定deployment的詳細資料         |
    | kubectl set image deploy/ <deployment-name> <pod-name>: <image-path>: <version> |  將deployment管理的pod升級到特定image版本   |
    |            kubectl edit deploy <deployment-name>             |           編輯特定deployment物件            |
    |       kubectl rollout status deploy <deployment-name>        |        查詢目前某deployment升級狀況         |
    |       kubectl rollout history deploy <deployment-name>       |     查詢目前某deployment升級的歷史紀錄      |
    |        kubectl rollout undo deploy <deployment-name>         |            回滾Pod到先前一個版本            |
    | kubectl rollout undo deploy <deployment-name> --to-revision=n |            回滾Pod到某個特定版本            |

###### Health check

- ###### 透過`定期發送一個 HTTP request 給 container` 的方式，來判斷目前 web app container 是否還正常運作。

  - 修改yaml, 在 Pod 中加入 Health check(livenessProbe)

    - **path**
      設定 health checks 要訪問的路徑
    - **port**
      指定我們要訪問的 port，這裡 port number 是 3000
    - **initialDelaySeconds**
      設定當 service 剛啟動時，要延遲幾秒再開始做 health check
    - **periodSeconds**
      代表每隔幾秒訪問一次，預設值為 `10秒`
    - **successThreshold**
      可以設置訪問幾次就代表目前 service 還正常運行
    - **failureThreshold**
      代表 service 回傳不如預期時，在 Kubernetes 放棄該 container 之前，會嘗試的次數，預設為`3`次。

  - ```yaml
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: hello-deployment
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: my-deployment
      template:
        metadata:
          labels:
            app: my-deployment
        spec:
          containers:
          - name: my-pod
            image: zxcvbnius/docker-demo:latest
            ports:
            - name: webapp-port
              containerPort: 3000
            livenessProbe:
              httpGet:
                path: /
                port: webapp-port
              initialDelaySeconds: 15
              periodSeconds: 15
              timeoutSeconds: 30  
              successThreshold: 1
              failureThreshold: 3
    ```

- ###### 相關文章

  - [還在用Replication Controller嗎？不妨考慮Deployment](https://ithelp.ithome.com.tw/articles/10194152)
  - [k8s Deployment 介紹](https://ithelp.ithome.com.tw/articles/10235654)

##### **Service** 

- ###### *Kubernetes Service 是個抽象化的概念，主要定義了邏輯上的一群 Pod 以及如何存取他們的規則。*

- ###### **Service 是 load balancer：**當發生 Replication 時，Service 會自動導流到比較空閑的 Application。（後面講 Deployment 的時候會再提到，可以先想像我們複製了一個 my-app，以防原本的 my-app 壞掉，而這兩個 my-app 都指向同一個 Service，Service 會導流到相對空閑的 my-app）

- ###### **Pod 跟 Service 的生命週期是分開的**

- ![Service](https://tachingchen.com/img/kubernetes-service/k8s-service-pod-access.jpg)

  - ##### 外部使用者會透過 Service 存取內部 Pod 以外 (路徑 1 -> 2)

  - ##### 同集群其他的 Pod 也有可能需要存取 (路徑 3 -> 2)。

  - ##### 兩條路徑的存取方式以及存取的 IP 位址有所不同

- ###### Service 作為中介層，避免使用者和 Pod 進行直接連線，除了讓我們服務維持一定彈性能夠選擇不同的 Pod 來處理請求外，某種程度上亦避免裸露無謂的 Port 而導致資安問題。

- ##### 主要元素:

  - ###### 服務元資料 (Metadata)

    - 簡單來說就是服務的名稱，讓其他人瞭解該服務的用途

      - ```
        metadata:
          name: service-example
        ```

  - 被存取的應用之標籤-

    - 每個 Pod 本身會帶有一至多個標籤，如何將使用者請求送到正確的 Pod，仰賴管理者設定的標籤是否得當。比方說，今天有 Nginx 以及 Apache 兩種網頁伺服器在運作，維運人員希望將流量導向至 Nginx，他們只要在 Pod 的 `spec.selector` 設定一組 `app: nginx` 的標籤，接著在 Service 內定義:

    - ```
      spec:
        selector:
          app: nginx
      ```

    - Service 便會根據定義檔內所設定的標籤，透過 [Label Selector](https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/) 找到對應的 Pod 後，建立相對應的 Iptable 規則。如此一來，當使用者請求送達 Kubernetes cluster 時，便會依照 Iptable 規則將封包繞送到實際執行的 Pod 內

  - 存取該服務的方式

    - 服務要開放給外界使用時，需要定義該服務的 Port、Protocol

    - ```
      spec:
        ports:
        	  # 讓維運人員瞭解該埠用途
          - name: http 
            # 對外部開放的埠號
            port: 80
            # 實際 Pod 所開放的埠號
            targetPort: 80
            # 該服務使用的協定目前有 TCP/UDP 兩種，預設為 TCP
            protocol: TCP
          - name: https
            port: 443
            targetPort: 443
            protocol: TCP
      ```

  - ##### 完整Yaml範例

    - ```yaml
      # service-example.yaml
      apiVersion: v1
      kind: Service
      metadata:
        name: hello-service
      spec:
        type: NodePort
        ports:
          - name: http-test1
            port: 80  #對應Container Port
            protocol: TCP
            nodePort: 30390
          - name: http-test2
            port: 3000
            protocol: TCP
            nodePort: 30391
        selector:
          app: my-deployment
      ```

  - ##### 建立 Kubernetes Service ` kubectl apply -f service-example.yaml service "service-example" created`

- ##### 產生Service

  - ```
     kubectl create -f ./my-service.yaml
    ```

- ##### 取得Service 清單

  - ```powershell
    > kubectl get services
    NAME                    TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                       AGE
    hello-service           NodePort    10.108.216.73   <none>        3000:30390/TCP                4m43s
    kubernetes              ClusterIP   10.96.0.1       <none>        443/TCP                       122m
    my-deployment-service   NodePort    10.99.112.135   <none>        3000:32726/TCP,80:31035/TCP   110m
    my-pod-service          NodePort    10.109.252.12   <none>        80:30447/TCP                  118m
    ```

- ##### 刪除Service

  - ```powershell
    > kubectl delete svc/hello-service
    service "hello-service" deleted
    ```

- ##### 取得Service Url

  - ```powershell
    > minikube service hello-service --url
    http://172.31.142.202:30390
    http://172.31.142.202:30391
    ```

- ##### Dynamic Cluster IP

  - ###### 在沒有指定Cluster IP的情形下，Service每次新建立都會重新給定IP

  - ```powershell
    > kubectl get svc
    NAME            TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                       AGE
    hello-service   NodePort    10.111.192.111   <none>        80:30390/TCP,3000:30391/TCP   7m13s
    kubernetes      ClusterIP   10.96.0.1        <none>        443/TCP                       137m
    
    > kubectl delete svc/hello-service
    service "hello-service" deleted
    
    > kubectl create -f ./my-service.yaml
    service/hello-service created
    
    >  kubectl get svc
    NAME            TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                       AGE
    hello-service   NodePort    10.103.80.198   <none>        80:30390/TCP,3000:30391/TCP   2s
    kubernetes      ClusterIP   10.96.0.1       <none>        443/TCP                       138m
    ```

- ##### NodePort 

  - ###### 預設的Service中，`Service可以指定的nodePort只有3000~32767`

  - 如果想要指定額外的Port號, 可以在創建Node時指定

    - 以 `minikube` 為例，當下次啟動minikube時我們可以加上`--extra-config=apiserver.ServiceNodePortRange={PORT_RANGE}`

    - ```
      minikube stop && minikube start --extra-config=apiserver.ServiceNodePortRange=1-50000
      ```

- ##### 參考

  - [Service Overview](https://godleon.github.io/blog/Kubernetes/k8s-Service-Overview/)


##### Ingress 

- ##### `Ingress` 負責給 `Service` 提供外部訪問的 `URL`、`SSL` 驗證、負載平衡、`HTTP`路由過濾等行為

- ##### 若將 [Service](https://kubernetes.io/docs/concepts/services-networking/service/) 圖像化，可以看到當多個 Service 同時運行時，Node 都需要有相對應的 port number 去對應相每個 [Service](https://kubernetes.io/docs/concepts/services-networking/service/) 的 port number

  - ![img](https://github.com/zxcvbnius/k8s-30-day-sharing/blob/master/Day19/describe-service.png?raw=true)

- ##### 通常雲端服務，每台機器都會配置屬於它自己的防火牆(firewall)。這也代表，不論**新增、或是刪除 [Service](https://kubernetes.io/docs/concepts/services-networking/service/) 物件，我們都必須額外調整防火牆的設定，port的管理也相對複雜**。

- ##### 若是使用 [Ingress](https://kubernetes.io/docs/concepts/services-networking/ingress/) ，我們**只需開放一個對外的 port number，[Ingress](https://kubernetes.io/docs/concepts/services-networking/ingress/) 可以在設定檔中設置不同的路徑，決定要將使用者的請求傳送到哪個 Service 物件**

  - ![img](https://github.com/zxcvbnius/k8s-30-day-sharing/blob/master/Day19/describe-ingress.png?raw=true)

- ##### 除了讓運維者無需維護多個 port 或頻繁更改防火牆(firewall)外，`可以自設條件`的功能也使得請求的導向更加彈性

- ##### Example 1：將不同路徑的請求對應到不同的 Service 物件

  - ```
    apiVersion: extensions/v1beta1
    kind: Ingress
    metadata:
      name: example-1
    spec:
      rules:
      - http:
          paths:
          - path: /test
            backend:
              serviceName: test
              servicePort: 80
    ```

  - ##### 由上述設定檔我們可以知道，

    - ###### **目前 Ingress 支援的 API 版本** 是 `extensions/v1beta1`

    - ###### 該設定檔中設定了一個規則：Node 收到流量之後，判斷流量路徑，若是請求路徑為 `/test` 則該流量將**導到名稱為 test 的 Service 物件**

- ##### Example 2：將不同 domain name 的請求對應到不同的 Service 物件

  - ```
    apiVersion: extensions/v1beta1
    kind: Ingress
    metadata:
      name: example-2
    spec:
      rules:
      - host: helloworld-v1.example.com
        http:
          paths:
          - path: /
            backend:
              serviceName: hellworld-v1
              servicePort: 80
      - host: helloworld-v2.example.com
        http:
          paths:
          - path: /
            backend:
              serviceName: helloworld-v2
              servicePort: 80
    ```

- ##### 在使用 `Ingress` 之前，**一定需要先運行 [Ingress Controller](https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/)**，筆者本身習慣使用 [Nginx](https://www.nginx.com/products/nginx/kubernetes-ingress-controller) 作為平常使用的 `Ingress Controller`

  - `Ingress Controller` 扮演著與 `K8S API` 溝通的腳色，並隨時注意 `Ingress` 規則是否有變化、如果有變化則自行更新規則。

  - ###### 目前主要支援兩種型別 [GCE](https://git.k8s.io/ingress-gce/README.md) 與 [Nginx](https://git.k8s.io/ingress-nginx/README.md)

- 參考

  - [在 Kubernetes 中實現負載平衡 - Ingress Controller](https://ithelp.ithome.com.tw/articles/10196261)

##### Flannel

##### Labels

- ##### **一對具有辨識度的key/value**，以下面為例

  - ###### "release" : "stable"，"release" : "qa"

  - ###### "enviroment": "dev"，"enviroment": "production"

  - ###### "tier": "backend", "tier": "frontend"

  - ###### "department": "enginnerting", "department": "marketing", "department": "finance"

- ##### 特點：

  - ###### 每個物件可以同時擁有許多個labels(multiple labels)

  - ###### 可以透過 [Selector](https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/)，幫我們縮小要尋找的物件。

  - ###### 目前 API 提供不再只是一個 **key對應一個value(Equality-based requirement)的關係**，我們也可以使用 [matchExpressions](https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/) 來設定更有彈性的`Labels`。

- ##### Annotations 

  - ##### 如果是**沒有識別用途的標籤**，Kubernetes 也提供了我們一個 [Annotations](https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/) 元件。以 Pod 為例，我們可以在Pod的 [Annotations](https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/) 紀錄該 Pod的發行時間，發行版本，聯絡人email等。

  - ##### [Annotations](https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/) 主要是方便開發者、以及系統管理者管理上的方便，不會直接被 Kubernetes使用。

  - EX.

    - ```
      apiVersion: v1
      kind: Pod
      metadata:
        name: my-pod
        labels:
          app: webserver
          tier: backend   
        annotations:
          version: latest
          release_date: 2017/12/28
          contact: zxcvbnius@gmail.com
      spec:
        containers:
        - name: pod-demo
          image: zxcvbnius/docker-demo
          ports:
          - containerPort: 3000     
      ```

  - ##### 動態新增 Labels

    - 透過 `kubectl label`的指令，來為我們的 Pod 新增標籤

    - ```
      $ kubectl label pods my-pod env=production
          pod "my-pod" labeled
      
          $ kubectl get pods my-pod --show-labels
      NAME      READY     STATUS    RESTARTS   AGE       LABELS
      my-pod    1/1       Running   0          1h        app=webserver,env=production,tier=backend
      ```

  - ##### 透過幫每個 [Node](https://kubernetes.io/docs/concepts/architecture/nodes/) 貼標籤讓 [Pod](https://kubernetes.io/docs/concepts/workloads/pods/pod/) 運行在特定的 Node 上

    - 在 [Node](https://kubernetes.io/docs/concepts/architecture/nodes/) 上貼上標籤
    - 在 [Pod](https://kubernetes.io/docs/concepts/workloads/pods/pod/) 新增 `nodeSelector` 定義
    - ![img](https://github.com/zxcvbnius/k8s-30-day-sharing/blob/master/Day10/kubectl-add-node-selector.png?raw=true)

- 參考

  - [Kubernetes 30天學習筆記 - 詳細操作說明](https://ithelp.ithome.com.tw/users/20103753/ironman/1590)
  - [了解 K8S 的 Ingress](https://ithelp.ithome.com.tw/articles/10224065)

#### Secret 

- ######  [Kubernetes](https://kubernetes.io/) 提供開發者一種存放敏感資訊的方式

- ###### [Kubernetes](https://kubernetes.io/) 本身也使用**相同的機制( secrets mechanism) 存放 access token**，限制 API 的存取權限，確保不會有外部服務隨意操作 Kubernetes API。

- ##### 常見的使用方式

  - ###### 將 [Secrets](https://kubernetes.io/docs/concepts/configuration/secret/) 當成 `環境變數(environment variables)` 使用

  - ###### 將 [Secrets File](https://kubernetes.io/docs/concepts/configuration/secret/) 掛載(mount) 在 [Pod](https://kubernetes.io/docs/concepts/workloads/pods/pod/) 某個檔案路徑底下使用

  - ###### 將這些 sensitive data 統一存放在某一個 Docker Image 中，並將這個 Image 存放在`私有的 Image Registry` 中，透過 `image pull` 下載到 Kubernetes Cluster 中，讓其他 Pods 存取。

#### Namespace 

- ##### 在同一個 K8s 中，每個 Namespaces 的名稱都是要獨特的

- ##### 當一個 Namespaces 被刪除時，在該 Namespace 裡的所有物件也會被刪除

- ##### 可以透過 Resource Quotas 限制一個 Namespaces 所可以存取的資源



# Load Balance

- ###### Server Load Balance（SLB）是最早發展的一塊，其功能在於將企業內多部伺服器的負載量作平衡，將過大流量轉向相同功能的其他伺服器，讓每一部伺服器工作量維持平均，保持服務不因流量過大而變慢或中斷。SLB主要應用在入口端的網頁分流，透過SLB轉址到各個伺服器，平均分配網路流量。

- ###### 假設今天開了五台機器，我們希望能夠分流這 1000 人的需求，但不可能讓 DNS 同時對到五台機器，所以在架構上，必須在這五台機器之前架設一台 load balance 的機器來做分流。設定時有幾個重點

  1. 設定後端機器的IP，這樣Load Balance才知道要管理那幾台機器。
  2. 設定判斷後端機器是否正常的方法。通常有幾種方式來判斷後端的機器是否處於正常服務狀態，例如透過SSH、HTTP(S)等，來判斷後端是否是處於一個正常狀態。在Web開發時，建議使用HTTP(S)方式來判斷，畢竟機器正常（SSH可連入），不代表我們的Server程式也正常（HTTP可連入）。因應檢測結果，不正常的後端機器，Load Balance會將其排除於服務外，並持續檢測，直到判斷為正常後，再重新加入到服務內。
  3. HTTPS的設定，因為HTTPS不單單是放在後端機器了，所以在Load Balance上，也必需針對HTTPS做設定。
  4. 設定分流的基準，可以用CPU或記憶體的用量（usage），來判斷這台後端機器是否是忙錄，決定要不要將使用者導向這台後端機器。

  

- ![Untitled.png](https://lh3.googleusercontent.com/p09zvycU01_6-rqqBVtTcn_csWuIc0J7UJ8_iCxsM9yFtT3wTvxk_y6ozXYUz2ElM5VVURogGoqmw1wJSI1wm7hPANT9WXombRGYTTjkRTP6fkzELG88eZg7tpP3Rinh13CvmZnq)

- ##### 實現策略

  - ##### 均勻派發(Even Task Distribution Scheme)

    - ###### 任務將均勻地派發到所有的伺服器進程。在實現時，可以使用隨機派發或者輪流派發(Round Robin)。實際上，由於進程部署環境的不同，其處理能力一般不同，任務處理時間也不盡相同。因此均勻派發的策略並不能很好地將任務負載均灘到各個進程中

    - ![img](http://i2.kknews.cc/uu0tUfeXFOAoHvmLCz4qoITdy_VYztv62VR6bzU/0.jpg)

  - ##### 加權派發(Weighted Task Distribution Scheme)

    - 賦予伺服器進程一個權值，即不同的進程會接受不同數量的任務，具體數量為權值確定。

    - ###### 例如，三個進程的處理任務的能力比率為3:3:2，那麼可以賦予這三個進程3:3:2的權值，即每8個任務中，3個發派給第一個進程，3個發派給第二個進程，2個分派給第三個進程。

    - ![img](http://i1.kknews.cc/v0O0M9oaV7uUDso_XGKRTbjEKcSWKcC-Gf8MRf0/0.jpg)

  - ##### 粘滯會話(Sticky Session Scheme)

    - ###### 前面兩種負載均衡策略並沒有考慮任務之間的依賴關係，在實際中，後面的任務處理常常會依賴於前面的任務。

    - ###### 例如，對於同一個登錄的用戶的請求，用戶購買的請求依賴於用戶登錄的請求，如果用戶的登錄信息保存在進程1中，那麼，如果購買請求被分派到進程2或者進程3，那麼購買請求將不能正確處理。這種請求間的依賴關係也稱為粘滯會話(Sticky Session)，負載均衡策略需要考慮粘滯會話的情況。

    - ![img](http://i1.kknews.cc/p66WWuXWi4C4XQFU_HZheEFM9JgWDBKT4QhFGJg/0.jpg)

    - ###### 粘滯會話的另一種處理策略是使用資料庫或者緩存，將所有會話數據存儲到資料庫或者緩存中。集群內所有進程都可以通過訪問資料庫或者緩存來獲取會話數據，進程內存都不保存會話數據，這樣，負載均衡器便可以使用前面介紹的策略來派發任務。

  - ##### 均勻任務隊列派發(Even Size Task Queue Distribution Scheme)

    - ###### 在均勻隊列派發策略下，負載均衡器為每個進程都創建一個大小相等的任務隊列，這些任務隊列包含了對應進程需要處理的任務。任務處理快的進程，其隊列也會減少得快，這樣負載均衡器會派發更多的任務給這個進程;相應地，任務處理慢的進程，其隊列也會減少得慢，這樣負載均衡器會派發更少的任務給這個進程。因此，通過這些任務隊列，負載均衡器在派發任務時將進程處理任務的能力因素考慮了進去。

    - ![img](http://i1.kknews.cc/DTiF0vP8KA8E6qdEK2uyACziS-85ZksxEuXlb0I/0.jpg)

  - #####  單一隊列(Autonomous Queue Scheme)

    - ###### 單一隊列策略中，實際上並沒有負載均衡器的存在。所有的伺服器進程從隊列中取出任務執行，如果某個進程出現宕機的情況，那麼其他進程仍然可以繼續執行任務。這樣一來，任務隊列並不需要知道服務進程的情況，只需要服務進程知道自己的任務隊列，並不斷執行任務即可。

    - ###### 單一隊列策略實際上也考慮到進程的處理能力，進程處理任務得越快，其從隊列取出任務的速度也越快。

    - ![img](http://i2.kknews.cc/CIterf9UhogBtAHZUKbtNclQ7DQq6SvV0Z1RNlU/0.jpg)

- ##### 作法

  - ##### persistence

    - ###### 通常我們在寫程式時，都是以使用者會連到同一台機器上來撰寫的，正常來說，當這個使用者的 session 存在時，我們會將他導到同一台機器，直到 session 失效。

      - ###### 這種分流會遇到的問題是：當服務請求變的更大時，我們加開的機器，並不會將原本的使用者分流過來，假設目前已經有 1000 人在前面五台機器上，這時候你開了第六台，前面的 1000 人並不會被分流過來，而是第 1001 人才會。

      - ###### 也因為 session 都在固定的機器上，如果今天使用者的 session 在第二台機器上，當第二台機器發生故障被導向第四台的時候，則使用者會被重新登入報錯

  - ##### affinity

    - ###### 根據機器的忙碌程度來決定將使用者導向何處。

      - ###### 那這種作法一樣會有問題發生，例如有使用者透過第一台機器登入了，但因為分流機制將第二個查詢動作給了第五台機器，那第五台機器沒有這個使用者的 session，也就會查不到資料了。很明顯的，各機器有各自的 session，如果要解決這個問題，就必須設計共有的 session 機制。

  - ##### Cluster

    - ###### 可以將多台 server 連在一起，通常要看使用的伺服器有沒有這個功能，一般來說依照設定就可以完成，也因為叢級功能基本上就有「共用 session」的功能了。

    - ###### 也可以用第三方服務來設計「共用 session」，比方 Memcached、AWS dynamoDB。

 



#  API Getway

- ##### 微服務架構中的唯一入口，它提供一個單獨且統一的API入口用於訪問內部一個或多個API。

- 具有**身份驗證，監控，負載均衡，快取，請求分片與管理**，靜態響應處理等

- ##### API閘道器方式的核心要點是，所有的客戶端和消費端都通過統一的閘道器接入微服務，在閘道器層處理所有的非業務功能

- ##### 通常，閘道器也是提供REST/HTTP的訪問API。服務端通過API-GW註冊和管理服務。

## **Ocelot**

- ##### 用.NET Core實現並且開源的API閘道器，它功能強大，包括了：路由、請求聚合、服務發現、認證、鑑權、限流熔斷、並內建了負載均衡器與Service Fabric、Butterfly Tracing整合。

- **工作流程**

  - **基本整合**

    - ######  根據configuration.json中配置內容，把接收所有的客戶端請求，路由到對應的下游伺服器進行處理，再將請求結果返回。而這個上下游請求的對應關係也被稱之為路由。

    - ![.NET Core 微服務—API閘道器(Ocelot) 教程 [一]](https://i.iter01.com/images/91882e5808b330a088525cb2ff2d4c99d8b471d70dc895f9c6c079277aa86ad3.png)

  -  **整合IdentityServer**

    - ###### 當我們涉及到授權認證的時候，我們可以跟Identity Server進行結合。當閘道器需要請求認證資訊的時候會與Identity Server伺服器進行互動來完成。

    - ![.NET Core 微服務—API閘道器(Ocelot) 教程 [一]](https://i.iter01.com/images/11ab960e17a64720aaeb26a4a55b9dab00108b48c2d425cde0e251358f2a4562.png)

  - **閘道器叢集配置**

    - ###### 可以部署多臺Ocelot閘道器。當然這個時候在多臺閘道器前，你還需要一臺負載均衡器

    - ![.NET Core 微服務—API閘道器(Ocelot) 教程 [一]](https://i.iter01.com/images/03dfb62b8d55f9c65f89594b4b28785a720130593bcb2ea0562470df4671599e.png)

  - ##### 結合Consul服務發現

    - ###### Ocelot已經支援簡單的負載功能，當下遊服務存在多個結點的時候，Ocelot能夠承擔起負載均衡的作用。但是沒提供健康檢查，服務的註冊也只能通過手動在配置檔案裡面新增完成。這不夠靈活並且在一定程度下會有風險。這個時候我們就可以用Consul來做服務發現，它能與Ocelot完美結合。

    - ![.NET Core 微服務—API閘道器(Ocelot) 教程 [一]](https://i.iter01.com/images/43971b816c33ad6d9fd39e5b0766ca6ab9b4bf892a1ffadb24079f527cb0445d.png)

  - ##### 結合Service Fabric

    - ![.NET Core 微服務—API閘道器(Ocelot) 教程 [一]](https://i.iter01.com/images/640d4efeb9194ffb891b1025be49f34c9ef0834dc5051f2885cbc99ee7810419.png)

  - 

